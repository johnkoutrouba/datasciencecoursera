## Getting and Cleaning Data Course Project
This repo contains R code for downloading and processing data files to create a tidy data set for later analysis.  Thanks for taking a look.  If you happen to be evaluating my tidy data set and code, allow me to walk you through the steps I took to create it.  The headings correspond to the five steps outlined in the cource project instructions, which can be found here: https://class.coursera.org/getdata-016/human_grading/view/courses/973758/assessments/3/submissions.

## Step 0: Download and unzip data
The code starts with some housecleaning: it checks to see if the courseradatascience/getdata-016 directory exists off of your home directory, and kindly creates it for you if not.  To make references to file paths easier, it then sets the working directory to getdata-016.  Further, it loads any libraries that it will need later on.  The code uses download.file to download the data file from the course web site.  The file is compressed using ZIP, and must be extracted before the data can be read.  The code uses the unzip utility to extract the files into the working directory.
We are interested in eight files, and load them into data frames.  The largest files are the feature data files named X_train and X_test.  These are fixed width text files that contain 7352 and 2947 records of 561 data points, respectively.  The read.fwf function with parameters widths = rep(16, 561) and buffersize = 10 divides the data into 16-character fields 561 times and reads in the records 10-at-a-time (to avoid huge memory use).  I used the list of features (read from features.txt using read.table) to create column names for the data frame.  The remaining files of interest, y_train, y_test, subject_train, subject_test, and activity_labels are all read using the read.table function with default settings.
The first actual processing of the data occurs in this step.  To make the feature names friendlier to R, the open and close parenthesis characters ("(" and ")") were removed from the feature names and replaced with the tilde symbol ("~").  This processing did not change the data in any way, and was performed to make later steps more convenient.

## Step 1: Combine the train and test data sets
The first step in the project instructions is to combine the training and test sets.  The rbind function stacks the records for each set of data files to create full data files.  It was important to bind the training and test files in that order to preserve the ordering of the data.  The resulting data set contained 10,299 records.

## Step 2: Extract only the measurement for mean and standard deviation of the measurements
The main data set contains 561 features.  Our interest is for only the features that are means or standard deviations of the various measurements collected in the data set.  This was achieved using the subset function in R and feeding it the column indexes matching any instance of "mean" or "std" in the feature name using the grep function.  This reduced the number of features to 66 from the original 561.  Features comprised of averages across the sampling windows, such as "gravityMean," were excluded from the limited data set because they represent calculated values, and not averages of actual observations.
NOTE: At the end of this step, the subject and activity codes were joined, creating a final extracted data set of 68 variables for each of the 10,299 records.  This join might have been performed prior to the extraction of the desired features, but was performed afterward to convenience of data frame naming.

## Step 3: Create descriptive activity names
The list of activities provided in the y_train and y_test files were codes refering to one of six possible activities performed during the collection of the data.  An addition file called activity_labels provided friendlier, human-readable versions of the activity names.  I used the sqldf function to join the data set to this activity label list and replace the activity code with the nicer description.

## Step 4: Create descriptive variable names
The variable names provided in the feature list were systematically created using a set of codes to represent various attributes contained in each feature.  I used the sub function to replace the text inside the feature names with text that is easier to read and understand.  See the included code book for more information about the names.

## Step 5: Create a tidy dataset with averages for each subject/activity/variable combination
Unfortunately, each feature contains a combination of six variables: domain, component, sensor, transform, summary, and axis.  The domain can take the values "time" or "frequency."  The component takes the value "body" or "gravity."  The sensor takes the value "accelerometer" or "gyroscope."  The transform takes the value "jerk," "magnitude," or "no transform."  The summary takes the value "mean" or "standard deviation."  The axis takes the value "X," "Y," "Z," or "no axis."  In order to create a tidy dataset, these embedded values had to be split into a set of variables in separate columns.
The first step, however, was to create a long data set with one given variable per record.  I used the melt function to create this dataset, which contained 4 columns: subject, activity, variable (which contained the feature name), and value (the actual measurement).  The resulting set was 66 * 10299 = 679,734 records long.
Next, I created a series of six new variables using subsetting and the grep function to populate the appropriate values into each column.
I then removed the now-redundant variable column from the data set, leaving a set of eight variables and 679,734 recods.
Finally, I created a tbl object and used the group_by and summarise functions to calculate the means of each of the values of the resulting 10,440 combinations of the variables (30 subjects * 6 activities * 2 domains * 2 components * 2 sensors * 3 transform options * 2 summary methods * 4 axis options = 34,560 possibilities, but some combinations do not occur).

## Grading
The data file uploaded to the site is in a tab-delimited format.  You can read is using read.table("tidy_set") with the default options, assuming you have downloaded it to your current working directory.  This readme walks you through the processing performed on the file.  The code book contains the updated variable information.
Thanks for attention.
